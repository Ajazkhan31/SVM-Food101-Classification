{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food101 Classification with SVM\n",
    "\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/insignite/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/insignite/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/insignite/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/insignite/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/insignite/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/insignite/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/insignite/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/insignite/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/insignite/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/insignite/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/insignite/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/insignite/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "from PIL import Image\n",
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2grey\n",
    "from skimage import data, exposure\n",
    "from skimage.data import camera\n",
    "\n",
    "# Library for scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Library for LIBSVM\n",
    "from libsvm.svmutil import *\n",
    "from itertools import combinations\n",
    "\n",
    "# Library fror tensorflow and keras\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import keras\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 101 is main dataset, 102 is for testing with smaller dataset\n",
    "data_dir = \"/mnt/c/Users/nhmin/Downloads/food-101\"\n",
    "project_dir = os.getcwd()\n",
    "model_dir = \"/mnt/c/Users/nhmin/Downloads\"\n",
    "# Add to dictionary if want more class\n",
    "class_label = {\"pad_thai\" : 0, \"pho\" : 1, \"ramen\" : 2, \"spaghetti_bolognese\" : 3, \"spaghetti_carbonara\" : 4}\n",
    "high_params = [0,2,0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ultilities functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load_json function use to read in train.json and test.json\n",
    "# and only take data according to class_label variable\n",
    "def load_json(path):\n",
    "    final_data = dict()\n",
    "    # Load in json file to create dictionary: key = class label; value = file path\n",
    "    with open(path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    # Only get information from needed class\n",
    "    for label in class_label:\n",
    "        final_data.update({label : data.get(label)})\n",
    "    return final_data\n",
    "\n",
    "\n",
    "# Return np img size 227x227\n",
    "def get_image(path):\n",
    "    # image resize to 384x384\n",
    "    img = Image.open(path + \".jpg\")\n",
    "    resized_image = img.resize((227,227), Image.ANTIALIAS)\n",
    "    return np.array(resized_image)\n",
    "\n",
    "\n",
    "\n",
    "# Return class label (key) according to values\n",
    "# SVM return label of number so this function return\n",
    "# the corresponding label\n",
    "def get_key(val):\n",
    "    for key, value in class_label.items():\n",
    "        if(val == value): return key\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# Make a dataframe have all image name and its label\n",
    "def create_data(json_data):\n",
    "    # Create train data\n",
    "    file_names_list = []\n",
    "    label_list = []\n",
    "    for label in json_data.keys():\n",
    "        #file_names = os.listdir(data_dir + '/images/' + label)\n",
    "        file_names = json_data.get(label)\n",
    "        for file in file_names:\n",
    "            file = file.split('/')[1] + '.jpg'\n",
    "            file_names_list.append(file)\n",
    "            label_list.append(class_label.get(label))\n",
    "    # Create dataframe\n",
    "    data_df = pd.DataFrame({\n",
    "        'filename' : file_names_list,\n",
    "        'label' : label_list\n",
    "    })\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write accuracy record to csv\n",
    "def write_record_txt(path):\n",
    "    kernel = []\n",
    "    gamma = []\n",
    "    cost = []\n",
    "    acc = []\n",
    "    kernel_dict = {'0' :'Linear', '1': 'Polynomial', '2' : 'RBF' }\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split(' : ')\n",
    "            params = values[0].split(',')\n",
    "            kernel.append(kernel_dict.get(params[0][1:]))\n",
    "            gamma.append(params[1])\n",
    "            cost.append(params[2][:-1])\n",
    "            acc.append(values[1][:-2])\n",
    "    data_df = pd.DataFrame({\n",
    "        'Kernel' : kernel,\n",
    "        'Gamma' : gamma,\n",
    "        'Cost' : cost,\n",
    "        'Accuracy' : acc\n",
    "    })\n",
    "    data_df.to_csv('grid_search_record.csv')\n",
    "\n",
    "def grid_search(train_df, train_feature):\n",
    "    # Grid Search\n",
    "    kernel = [0,1,2]\n",
    "    cost = [2**(-5),2**(-3),2**(-1),1,2,2**3,2**5,2**7,2**9,2**11,2**13,2**15]\n",
    "    gamma = [2**(-15),2**(-13),2**(-11),2**(-9),2**(-7),2**(-5),2**(-3),2**(-1),1,2,2**3]\n",
    "    # Create all the combination O(n^3)\n",
    "    combine_list = [(k,g,c) for k in kernel for g in gamma for c in cost]\n",
    "    accuracy_record = {}\n",
    "    highest_acc = -1.0\n",
    "    highest_params = (0,0,0)\n",
    "    for i in range(0, len(combine_list)):\n",
    "        train_val_acc = svm_train(train_df['label'].tolist(), train_feature, '-s 0 -t %s -v 10 -g %s -c %s -b 1' % combine_list[i])\n",
    "        if(train_model > highest_acc):\n",
    "            highest_acc = train_model\n",
    "            highest_params = combine_list[i]\n",
    "        accuracy_record[combine_list[i]] = train_model\n",
    "    print('Highest Accuracy and Params: ')\n",
    "    print('(kernel, gamma, cost) : ',highest_params, ':', highest_acc)\n",
    "    write_record_txt(model_dir + '/grid_search_record.txt')\n",
    "    return highest_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram Oriented of Gradients (HOG) approach\n",
    "\n",
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking in an impage, perform HOG conversion\n",
    "# input: img path     output: nparray of HOG img\n",
    "def HOG_image(path):\n",
    "    np_image = load_img(path, target_size=(227,227))\n",
    "    # given 32x32 cell\n",
    "    image_feature, image_hog = hog(np_image, orientations=8, pixels_per_cell=(8, 8),\n",
    "        cells_per_block=(8, 8), block_norm = 'L2-Hys', visualize=True, multichannel=True)\n",
    "    return np.array(image_feature)\n",
    "\n",
    "# PCA to reduce the number of feature \n",
    "def pca_transform(image):\n",
    "    ss = StandardScaler()\n",
    "    image_ss = ss.fit_transform(image)\n",
    "    # Keep 90% of variance\n",
    "    pca = PCA(0.9)\n",
    "    image_pca = pca.fit_transform(image_ss)\n",
    "    return mage_pca\n",
    "\n",
    "# return a list of HOG converted img\n",
    "# input: train_df      output: list of HOG arrays\n",
    "def HOG_training(train_df):\n",
    "    train_data = []\n",
    "    size = len(train_df['filename'])\n",
    "    for i in range(0, size):\n",
    "        image = data_df['filename'][i]\n",
    "        label = data_df['label'][i]\n",
    "        path = data_dir + '/images/' + get_key(label) + '/' + image\n",
    "        HOG_data = HOG_image(path)\n",
    "        # this is where PCA apply. take off comment line if needed.\n",
    "        # but i believe PCA return an unequal list size on every image\n",
    "        #train_data.append(pca_transform(HOG_data))\n",
    "        train_data.append(HOG_data)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json file\n",
    "train_json = load_json(data_dir + '/meta/train.json')\n",
    "# Create dataframe for train\n",
    "train_df = create_data(train_json)\n",
    "\n",
    "# Convert img to HOG nparray\n",
    "hog_train_feature = HOG_training(train_df)\n",
    "\n",
    "#====Uncoment for Grid search==========\n",
    "#high_params = grid_search(train_df, hog_train_feature)\n",
    "#===========End block==================\n",
    "\n",
    "# b = 1 will give prob est\n",
    "# highest params for food-101 dataset\n",
    "# (0, 2, 0.5)\n",
    "hog_train_model = svm_train(train_df['label'].tolist(), hog_train_feature, '-s 0 -t %s -g %s -c %s -b 1' % (high_params[0], high_params[1], high_params[2]))\n",
    "svm_save_model('hog_food_classification.model', hog_train_model)\n",
    "print('Training finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load json file\n",
    "test_json = load_json(data_dir + '/meta/train.json')\n",
    "# Create dataframe for test\n",
    "test_df = create_data(test_json)\n",
    "\n",
    "# Convert img to HOG nparray\n",
    "hog_test_feature = HOG_training(test_df)\n",
    "\n",
    "# Load in SVM model\n",
    "hog_model = smv_load_model('hog_food_classification.model')\n",
    "\n",
    "# Predict\n",
    "hog_p_label, hog_p_acc, hog_p_val = svm_predict(test_df['label'].tolist(), hog_test_feature, hog_model, '-b 0')\n",
    "hog_acc, hog_mse, hog_scc = evaluations(test_df['label'].tolist(), hog_p_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning approach:\n",
    "\n",
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image through the new model to get features \n",
    "def create_feature(data_df, pre_model):\n",
    "    x_tmp = []\n",
    "    size = len(data_df['filename'])\n",
    "    print(\"total image: \", size)\n",
    "    for i in range(0,size):\n",
    "        image = data_df['filename'][i]\n",
    "        label = data_df['label'][i]\n",
    "        path = data_dir + '/images/' + get_key(label) + '/' + image\n",
    "        #image_np = get_image(path)\n",
    "        image_np = load_img(path, target_size=(227,227))\n",
    "        image_np = img_to_array(image_np)\n",
    "        image_np = np.expand_dims(image_np,axis=0)\n",
    "        image_np = imagenet_utils.preprocess_input(image_np)\n",
    "        x_tmp.append(image_np)\n",
    "    x = np.vstack(x_tmp)\n",
    "    features = pre_model.predict(x, batch_size=32)\n",
    "    last_layer = pre_model.layers[-1].output_shape\n",
    "    print(\"Reshape:\", features.shape[0], last_layer[1] * last_layer[2] * last_layer[3])\n",
    "    features_flatten = features.reshape((features.shape[0], last_layer[1] * last_layer[2] * last_layer[3]))\n",
    "    return features_flatten\n",
    "\n",
    "# Take in model hdf5 model and create a new model fpr transfer learning\n",
    "def transfer_learning(train_df):\n",
    "    # Load in hdf5 model\n",
    "    # model name can be anything and can be change\n",
    "    # i have mine as alexNet_best_weights.hdf5\n",
    "    model = tf.keras.models.load_model(model_dir + '/alexNet_best_weigths.hdf5')\n",
    "    # copy up to last convolutional layers\n",
    "    drop_layer = 0\n",
    "    for i in range(len(model.layers)-1, 0, -1):\n",
    "        name = model.layers[i].name.split('_')[0]\n",
    "        if(name == 'flatten'):\n",
    "            drop_layer+= 1\n",
    "            break\n",
    "        drop_layer += 1\n",
    "    new_model = tf.keras.Sequential()\n",
    "    for layer in model.layers[:-drop_layer]:\n",
    "        new_model.add(layer)\n",
    "    new_model.trainable = False\n",
    "    new_model.build((227,227,3))\n",
    "    new_model.summary()\n",
    "    train_feature = create_feature(train_df, new_model)\n",
    "    return train_feature, new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If you didn't run the HOG approach\n",
    "#===============Uncomment this block==================\n",
    "# # load json file\n",
    "# train_json = load_json(data_dir + '/meta/train.json')\n",
    "# # Create dataframe\n",
    "# train_df = create_data(train_json)\n",
    "#==================End block===========================\n",
    "\n",
    "tl_train_feature, new_model = transfer_learning(train_df)\n",
    "\n",
    "\n",
    "#====Uncoment for Grid search==========\n",
    "#high_params = grid_search(train_df, tl_train_feature)\n",
    "#===========End block==================\n",
    "\n",
    "# b = 1 will give prob est\n",
    "# highest params for food-101 dataset\n",
    "# (0, 2, 0.5)\n",
    "tl_train_model = svm_train(train_df['label'].tolist(), tl_train_feature, '-s 0 -t %s -g %s -c %s -b 1' % (high_params[0], high_params[1], high_params[2]))\n",
    "svm_save_model('tl_food_classification.model', tl_train_model)\n",
    "print('Training finished.')\n",
    "\n",
    "\n",
    "# Grid Search\n",
    "# Use small dataset to make search much faster\n",
    "# Transfer Learning\n",
    "# highest_params = grid_search(train_df, tl_train_feature)\n",
    "# HOG\n",
    "# highest_params = grid_search(train_df, hog_train_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you didn't run HOG test\n",
    "#==============Uncomment this block===================\n",
    "# # load json file\n",
    "# test_json = load_json(data_dir + '/meta/train.json')\n",
    "# # Create dataframe for test\n",
    "# test_df = create_data(test_json)\n",
    "#===================End block=========================\n",
    "\n",
    "# Load in SVM model\n",
    "tl_model = smv_load_model('tl_food_classification.model')\n",
    "\n",
    "# Convert test images to features array\n",
    "tl_test_feature = create_feature(test_df, tl_model)\n",
    "\n",
    "\n",
    "# Predict\n",
    "tl_p_label, tl_p_acc, tl_p_val = svm_predict(test_df['label'].tolist(), tl_test_feature, tl_model, '-b 0')\n",
    "tl_acc, tl_mse, tl_scc = evaluations(test_df['label'].tolist(), tl_p_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
